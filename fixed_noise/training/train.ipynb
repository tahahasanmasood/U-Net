{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: tldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_folder (str): Path to the folder containing images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        self.img_paths = [os.path.join(img_folder, fname) for fname in os.listdir(img_folder) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = cv2.imread(img_path)  # Read image using OpenCV\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        # Convert image to float32 for safe addition\n",
    "        img = img.astype(np.float32)\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.normal(0, 100, img.shape).astype(np.float32)  # Mean 0, Std 20\n",
    "        noisy_img = img + noise\n",
    "\n",
    "        # Clip values to [0, 255] and convert back to uint8\n",
    "        noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img.astype(np.uint8))  # Apply transform to original image\n",
    "            noisy_img = self.transform(noisy_img)       # Apply transform to noisy image\n",
    "\n",
    "        return img, noisy_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (optional)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),   # Convert numpy array to PIL image\n",
    "    transforms.Resize((256, 256)),  # Resize image to 256x256\n",
    "    transforms.ToTensor(),  # Convert image to Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = CustomImageDataset(\"/home/data/taha/UNET-denoising/celebahq_preprocess/train\", transform=transform)\n",
    "val_dataset = CustomImageDataset(\"/home/data/taha/UNET-denoising/celebahq_preprocess/val\", transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print dataset size\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # input: 256x256x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 254x254x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 252x252x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 126x126x64\n",
    "\n",
    "        # input: 126x126x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 124x124x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 122x122x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 61x61x128\n",
    "\n",
    "        # input: 61x61x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 27x27x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 25x25x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 13x13x256\n",
    "\n",
    "        # input: 13x13x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 11x11x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 9x9x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 5x5x512\n",
    "\n",
    "        # input: 5x5x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 3x3x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 1x1x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(n_class=3)\n",
    "model = model.to(device)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "best_loss = float('inf') #set to positive infinity to ensure that the first validation loss encountered will always be considered an improvement\n",
    "counter = 0  # Counter to keep track of consecutive epochs with no improvement\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for train_clean_images, train_noisy_images in train_loader:\n",
    "        train_clean_images, train_noisy_images = train_clean_images.to(device), train_noisy_images.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        train_outputs = model(train_noisy_images)\n",
    "        # Find the Loss\n",
    "        train_loss = criterion(train_outputs, train_clean_images)\n",
    "        # Calculate gradients\n",
    "        train_loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the training loss\n",
    "        running_loss += train_loss.item()\n",
    "            \n",
    "    train_total_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_total_loss)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_clean_images, val_noisy_images in val_loader:\n",
    "            val_clean_images, val_noisy_images = val_clean_images.to(device), val_noisy_images.to(device)\n",
    "            val_op = model(val_noisy_images)\n",
    "            val_loss = criterion(val_op, val_clean_images)\n",
    "            val_running_loss += val_loss.item()\n",
    "        \n",
    "        val_total_loss = val_running_loss / len(val_loader)\n",
    "        val_losses.append(val_total_loss)\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_total_loss < best_loss:\n",
    "        best_loss = val_total_loss\n",
    "        counter = 0\n",
    "        # Save the model if needed\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "        # Check if training should be stopped\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_total_loss:.4f},  Validation Loss: {val_total_loss: .4f}, Best Loss: {best_loss: .4f}')\n",
    "\n",
    "    sampled_noisy_image = train_noisy_images[0:1:,:,:,:]\n",
    "    sampled_clean_image = train_clean_images[0:1:,:,:,]\n",
    "    sampled_train_output = train_outputs[0:1:,:,:,:]\n",
    "    save_image(sampled_noisy_image, 'trainnoisyimagebeforeunet.png')\n",
    "    save_image(sampled_clean_image, 'traintargetcleanimage.png')\n",
    "    save_image(sampled_train_output, 'traincleanimageafterunet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.plot(train_losses,label=\"train\")\n",
    "plt.plot(val_losses,label=\"val\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "img1 = mpimg.imread(\"/home/data/taha/UNET-denoising/full_task/trainnoisyimagebeforeunet.png\")\n",
    "img2 = mpimg.imread(\"/home/data/taha/UNET-denoising/full_task/traintargetcleanimage.png\")\n",
    "img3 = mpimg.imread(\"/home/data/taha/UNET-denoising/full_task/traincleanimageafterunet.png\")\n",
    "plt.subplot(1,3,1).set_title(\"Train Noisy Image-Unet i/p\")\n",
    "plt.imshow(img1)\n",
    "plt.subplot(1,3,2).set_title(\"Train Target Clean Image\")\n",
    "plt.imshow(img2)\n",
    "plt.subplot(1,3,3).set_title(\"Train Clean Image-Unet o/p\")\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
